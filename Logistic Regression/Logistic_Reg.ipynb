{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d510aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da314d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifer:\n",
    "    \n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,iterations=1000):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        return (1/(1+np.exp(-z)))\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        samples,features = X.shape\n",
    "        self.weights = np.zeros(features)\n",
    "        self.bias = 0 \n",
    "        \n",
    "        for _ in range(self.iterations):\n",
    "            \n",
    "            linear_model  = X @ self.weights + self.bias\n",
    "            \n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "            \n",
    "            dw = (1/samples) * np.dot(X.T,(y_pred-y))\n",
    "            db = (1/samples) * np.sum(y_pred-y)\n",
    "            \n",
    "            \n",
    "            self.weights = self.weights - self.learning_rate * dw\n",
    "            self.bias = self.bias - self.learning_rate * db\n",
    "            \n",
    "\n",
    "    def predict(self,X):\n",
    "        return np.where(self.sigmoid(X @ self.weights + self.bias)>=0.5,1,0)\n",
    "    \n",
    "    \n",
    "class Multiclass_Classifier:\n",
    "    \n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,iterations=10000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.classes = None\n",
    "        self.classifiers = {}\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        total_classes = np.unique(y)\n",
    "        self.classes = total_classes\n",
    "        \n",
    "        for clss in total_classes:\n",
    "            \n",
    "            y_binary = np.where(y==clss,1,0)\n",
    "            clf = BinaryClassifer(self.learning_rate,self.iterations)\n",
    "            clf.fit(X,y_binary)\n",
    "            self.classifiers[clss] = clf\n",
    "            \n",
    "    def predict(self,X):\n",
    "        \n",
    "        total_classes = self.classifiers.keys()\n",
    "        predictions = []\n",
    "        \n",
    "        for cl in total_classes:\n",
    "            proba = self.classifiers[cl].predict(X)\n",
    "            predictions.append(proba)\n",
    "            \n",
    "        predictions = np.array(predictions)\n",
    "        predictions = predictions.T\n",
    "        \n",
    "        return self.classes[np.argmax(predictions,axis=1)]\n",
    "\n",
    "class Softmax_Classifier:\n",
    "    def __init__(self,iterations=10000,learning_rate=0.01):\n",
    "        \n",
    "        self.iterations = iterations \n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.classes = None \n",
    "        \n",
    "    def softmax_(self,z):\n",
    "        \n",
    "        z-= np.max(z,axis=1,keepdims=True)\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z/np.sum(exp_z,axis=1,keepdims=True)\n",
    "        \n",
    "    def one_hot_encoding(self,y,num_classes):\n",
    "        \n",
    "        one_hot_matrix = np.zeros((len(y),num_classes))\n",
    "        one_hot_matrix[np.arange(len(y)),y]=1\n",
    "        return one_hot_matrix\n",
    "        \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        samples,features = X.shape\n",
    "        \n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        classes_to_index = {cls:index for index,cls in enumerate(self.classes)}\n",
    "        \n",
    "        y_index = np.array([classes_to_index[i] for i in y])\n",
    "        \n",
    "        y_one_hot = self.one_hot_encoding(y_index,n_classes)\n",
    "        \n",
    "        self.weights = np.zeros((features,n_classes))\n",
    "        self.bias = np.zeros((1,n_classes))\n",
    "        \n",
    "        for _ in range(self.iterations):\n",
    "            \n",
    "            Z = X @ self.weights + self.bias\n",
    "            y_pred = self.softmax_(Z)\n",
    "            \n",
    "            \n",
    "            dw = (1/samples) * (X.T) @ (y_pred - y_one_hot)\n",
    "            db = (1/samples) * np.sum(y_pred-y_one_hot)\n",
    "            \n",
    "            self.weights-= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "    def predict(self,X):\n",
    "        pred = X @ self.weights + self.bias \n",
    "        proba = self.softmax_(pred)\n",
    "        predictions = np.argmax(proba,axis=1)\n",
    "        return self.classes[predictions]        \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a443926",
   "metadata": {},
   "source": [
    "### Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e66485f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4b956ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5772/3265970859.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return (1/(1+np.exp(-z)))\n"
     ]
    }
   ],
   "source": [
    "model = BinaryClassifer()\n",
    "\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9c873a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5772/3265970859.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return (1/(1+np.exp(-z)))\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "15290a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8df2a3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8788a907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05263157894736842"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1f424",
   "metadata": {},
   "source": [
    "### MultiNomial Logistic Regression OvR(One-vs-Rest) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab7c5534",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "da3aa36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5201668984700976"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Multiclass_Classifier()\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9832482",
   "metadata": {},
   "source": [
    "### Using Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "814872e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5618915159944368"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Softmax_Classifier()\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2918c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9258c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
